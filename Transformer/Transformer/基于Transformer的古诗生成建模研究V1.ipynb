{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 数据预处理函数\n",
    "def preprocess_data_char_level(file_path, seq_length):\n",
    "    \"\"\"\n",
    "    读取并预处理诗词数据，按字符切分，过滤掉过短的诗。\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    \n",
    "    # 提取诗词内容（去除标题和空行）\n",
    "    poems = []\n",
    "    current_poem = []\n",
    "    for line in data.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line.startswith('[') and line.endswith(']'):  # 跳过标题行\n",
    "            if current_poem:\n",
    "                poems.append(''.join(current_poem))\n",
    "                current_poem = []\n",
    "        elif line:  # 只处理非空行\n",
    "            # 去除标点符号外的特殊字符\n",
    "            cleaned_line = re.sub(r'[^\\u4e00-\\u9fa5，。！？、；：]', '', line)\n",
    "            current_poem.append(cleaned_line)\n",
    "    \n",
    "    if current_poem:  # 添加最后一首诗\n",
    "        poems.append(''.join(current_poem))\n",
    "    \n",
    "    # 过滤掉过短的诗\n",
    "    poems = [poem for poem in poems if len(poem) >= seq_length]\n",
    "    print(f\"预处理后的诗词数量: {len(poems)}\")\n",
    "    print(f\"示例诗词: {poems[0]}\")  # 打印第一首诗作为示例\n",
    "    return poems\n",
    "\n",
    "\n",
    "\n",
    "# 创建字符级别的词表\n",
    "\n",
    "def create_vocab(poems):\n",
    "    \"\"\"\n",
    "    创建字符级别的词表。\n",
    "    \"\"\"\n",
    "    all_chars = ''.join(poems)\n",
    "    unique_chars = sorted(list(set(all_chars)))  # 排序以保证一致性\n",
    "    # 添加特殊标记\n",
    "    unique_chars = ['<PAD>', '<UNK>'] + unique_chars\n",
    "    char2idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "    idx2char = {idx: char for idx, char in enumerate(unique_chars)}\n",
    "    print(f\"词表大小: {len(char2idx)}\")\n",
    "    return char2idx, idx2char\n",
    "\n",
    "\n",
    "# 数据集定义\n",
    "\n",
    "class CharLevelDataset(Dataset):\n",
    "    def __init__(self, poems, char2idx, seq_length):\n",
    "        self.data = []\n",
    "        self.char2idx = char2idx\n",
    "        self.seq_length = seq_length\n",
    "        self.unk_idx = char2idx.get('<UNK>', 0)\n",
    "\n",
    "        for poem in poems:\n",
    "            # 将字符转换为索引，未知字符用UNK代替\n",
    "            encoded = [char2idx.get(char, self.unk_idx) for char in poem]\n",
    "            # 生成训练样本\n",
    "            for i in range(len(encoded) - seq_length):\n",
    "                self.data.append((encoded[i:i + seq_length], encoded[i + 1:i + seq_length + 1]))\n",
    "        \n",
    "        print(f\"生成的数据样本数量: {len(self.data)}\")\n",
    "        if len(self.data) > 0:\n",
    "            print(f\"样本示例 - 输入: {self.data[0][0]}, 输出: {self.data[0][1]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Transformer 模型定义\n",
    "\n",
    "class CharLevelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, ff_dim, seq_length, dropout=0.1):\n",
    "        super(CharLevelTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = torch.zeros(1, seq_length, embedding_dim)  # 改为普通 Tensor\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # 动态扩展位置编码以匹配输入序列长度\n",
    "        if x.size(1) > self.positional_encoding.size(1):\n",
    "            extra_positional_encoding = torch.zeros(\n",
    "                1, x.size(1) - self.positional_encoding.size(1), self.positional_encoding.size(2),\n",
    "                device=x.device  # 确保扩展部分与输入张量在同一设备\n",
    "            )\n",
    "            self.positional_encoding = torch.cat([self.positional_encoding.to(x.device), extra_positional_encoding], dim=1)\n",
    "\n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :].to(x.device)\n",
    "        y = self.embedding(y) + self.positional_encoding[:, :y.size(1), :].to(x.device)\n",
    "\n",
    "        # Transformer\n",
    "        tgt_mask = self.generate_square_subsequent_mask(y.size(1)).to(x.device)\n",
    "        out = self.transformer(x, y, tgt_mask=tgt_mask)\n",
    "\n",
    "        # 输出层\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def generate_square_subsequent_mask(self, size):\n",
    "        \"\"\"\n",
    "        生成用于 Transformer 的解码器掩码，防止看到未来的时间步。\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "def plot_metrics(metrics_path, losses, perplexities, accuracies, inference_times):\n",
    "    \"\"\"\n",
    "    绘制训练过程中的指标曲线，包括 Loss、PPL、Accuracy 和推理时间。\n",
    "    \"\"\"\n",
    "    # 绘制 Loss 曲线\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(losses) + 1), losses, label='Loss', color='blue')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(metrics_path, 'loss_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 绘制 PPL 曲线\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(perplexities) + 1), perplexities, label='PPL', color='green')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.title('Training Perplexity Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(metrics_path, 'ppl_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 绘制 Accuracy 曲线\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(accuracies) + 1), accuracies, label='Accuracy', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(metrics_path, 'accuracy_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 绘制推理时间曲线\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(inference_times) + 1), inference_times, label='Inference Time', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.title('Inference Time Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(metrics_path, 'inference_time_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"所有指标曲线已保存到 {metrics_path}\")\n",
    "    \n",
    "from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "def plot_confusion_matrix_and_roc(metrics_path, y_true, y_pred, num_classes, max_classes=10):\n",
    "    \"\"\"\n",
    "    绘制混淆矩阵和 ROC 曲线，限制最多显示 max_classes 个类别。\n",
    "    \"\"\"\n",
    "    # 限制类别数量\n",
    "    if num_classes > max_classes:\n",
    "        print(f\"类别数量过多（{num_classes}），仅绘制前 {max_classes} 个类别。\")\n",
    "        y_true = np.clip(y_true, 0, max_classes - 1)\n",
    "        y_pred = np.clip(y_pred, 0, max_classes - 1)\n",
    "        num_classes = max_classes\n",
    "\n",
    "    # 绘制混淆矩阵\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(num_classes))\n",
    "    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(os.path.join(metrics_path, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 绘制 ROC 曲线\n",
    "    # ROC 曲线需要二分类问题，这里以每个类别计算一条 ROC 曲线\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    y_true_one_hot = np.eye(num_classes)[y_true]  # 将 y_true 转换为 one-hot 编码\n",
    "    y_pred_one_hot = np.eye(num_classes)[y_pred]  # 将 y_pred 转换为 one-hot 编码\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_one_hot[:, i], y_pred_one_hot[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # 绘制所有类别的 ROC 曲线\n",
    "    plt.figure()\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(os.path.join(metrics_path, 'roc_curve.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"混淆矩阵和 ROC 曲线已保存到 {metrics_path}\")\n",
    "\n",
    "\n",
    "def generate_text(model, start_text, char2idx, idx2char, seq_length, length, poem_type, temperature=0.8, device='cuda'):\n",
    "    \"\"\"\n",
    "    使用训练好的模型生成文本，确保标点符号只出现在行尾\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    unk_idx = char2idx.get('<UNK>', 0)\n",
    "    \n",
    "    # 初始化输入序列\n",
    "    input_seq = [char2idx.get(char, unk_idx) for char in start_text]\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 标题\n",
    "    generated_text = start_text + \"\\n\\n\"  # 标题和正文之间添加换行\n",
    "\n",
    "    # 控制行数和每行字数\n",
    "    num_lines = 4 if \"绝句\" in poem_type else 8  # 绝句4行，律诗8行\n",
    "    line_length = 5 if \"五言\" in poem_type else 7  # 五言5字，七言7字\n",
    "    punctuation = ['，', '。']  # 可用的标点符号\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for line_idx in range(1, num_lines + 1):  # 从1开始计数以便判断单双数行\n",
    "            line = \"\"\n",
    "            for char_idx in range(line_length):  # 控制每行的字数\n",
    "                tgt_input = input_seq[:, -seq_length:]  # 限制tgt_input的长度不超过seq_length\n",
    "                output = model(input_seq, tgt_input)\n",
    "                \n",
    "                # 只取最后一个时间步的输出\n",
    "                output = output[:, -1, :] / temperature\n",
    "                probs = torch.softmax(output, dim=-1)\n",
    "                \n",
    "                # 在整个诗句生成过程中完全禁止生成标点\n",
    "                for p in punctuation:\n",
    "                    if p in char2idx:\n",
    "                        probs[0, char2idx[p]] = 0\n",
    "                # 重新归一化概率\n",
    "                probs = probs / probs.sum()\n",
    "                \n",
    "                # 从概率分布中采样\n",
    "                next_char_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "                next_char = idx2char[next_char_idx]\n",
    "                \n",
    "                line += next_char\n",
    "                input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]], dtype=torch.long).to(device)], dim=1)\n",
    "            \n",
    "            # 在行尾手动添加标点符号（不在模型生成范围内）\n",
    "            if line_idx % 2 == 1:  # 单数行（1、3、5、7）\n",
    "                line += \"，\"\n",
    "            else:  # 双数行（2、4、6、8）\n",
    "                line += \"。\"\n",
    "            \n",
    "            generated_text += line + \"\\n\"\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# 模型训练函数\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "def train_transformer(data_path, model_save_path, metrics_path, seq_length=20, embedding_dim=128, num_heads=4, \n",
    "                      num_layers=2, ff_dim=512, epochs=20, batch_size=64, lr=0.001, device='cuda'):\n",
    "    # 数据预处理\n",
    "    poems = preprocess_data_char_level(data_path, seq_length)\n",
    "    char2idx, idx2char = create_vocab(poems)\n",
    "    dataset = CharLevelDataset(poems, char2idx, seq_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 模型初始化\n",
    "    model = CharLevelTransformer(len(char2idx), embedding_dim, num_heads, num_layers, ff_dim, seq_length).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 训练指标\n",
    "    losses = []\n",
    "    perplexities = []\n",
    "    accuracies = []\n",
    "    inference_times = []\n",
    "\n",
    "    # 用于记录所有真实标签和预测标签\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # 训练\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for x, y in progress_bar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, y[:, :-1])\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 计算准确率\n",
    "            predictions = output.argmax(dim=-1)\n",
    "            correct = (predictions == y[:, 1:]).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += y[:, 1:].numel()\n",
    "\n",
    "            # 记录真实标签和预测标签\n",
    "            all_labels.extend(y[:, 1:].cpu().numpy().flatten())  # 展平真实标签\n",
    "            all_predictions.extend(predictions.cpu().numpy().flatten())  # 展平预测标签\n",
    "\n",
    "            progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = total_correct / total_samples\n",
    "        perplexity = np.exp(avg_loss)  # PPL 是 Loss 的指数形式\n",
    "        epoch_end_time = time.time()\n",
    "        inference_time = epoch_end_time - epoch_start_time\n",
    "\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        perplexities.append(perplexity)\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, PPL: {perplexity:.4f}, Inference Time: {inference_time:.2f}s\")\n",
    "\n",
    "    # 保存模型和词表\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    torch.save({'char2idx': char2idx, 'idx2char': idx2char}, \n",
    "               model_save_path.replace('.pth', '_vocab.pth'))\n",
    "    print(f\"模型和词表已保存到 {model_save_path}\")\n",
    "\n",
    "    # 绘制并保存曲线\n",
    "    os.makedirs(metrics_path, exist_ok=True)\n",
    "    plot_metrics(metrics_path, losses, perplexities, accuracies, inference_times)\n",
    "\n",
    "    # 绘制混淆矩阵和 ROC 曲线\n",
    "    plot_confusion_matrix_and_roc(metrics_path, all_labels, all_predictions, num_classes=len(char2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "预处理后的诗词数量: 27\n",
      "示例诗词: 海畔尖山似劒铓，秋来处处割愁肠。若为化得身千亿，散上峰头望故乡。\n",
      "词表大小: 632\n",
      "生成的数据样本数量: 604\n",
      "样本示例 - 输入: [357, 407, 179, 188, 54, 95, 579, 631, 433, 300, 147, 147, 94, 240, 458, 2, 480, 18, 98, 223], 输出: [407, 179, 188, 54, 95, 579, 631, 433, 300, 147, 147, 94, 240, 458, 2, 480, 18, 98, 223, 538]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|████████████████████████████████████████████████████████████| 76/76 [00:01<00:00, 47.77it/s, Loss=2.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Loss: 4.7145, Accuracy: 0.2366, PPL: 111.5567, Inference Time: 1.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|███████████████████████████████████████████████████████████| 76/76 [00:01<00:00, 53.23it/s, Loss=0.825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Loss: 1.7007, Accuracy: 0.8050, PPL: 5.4777, Inference Time: 1.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|███████████████████████████████████████████████████████████| 76/76 [00:01<00:00, 51.24it/s, Loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Loss: 0.5450, Accuracy: 0.9428, PPL: 1.7247, Inference Time: 1.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|███████████████████████████████████████████████████████████| 76/76 [00:01<00:00, 56.19it/s, Loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Loss: 0.2318, Accuracy: 0.9776, PPL: 1.2608, Inference Time: 1.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████████████████████████████████████████████████████| 76/76 [00:01<00:00, 53.81it/s, Loss=0.0749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Avg Loss: 0.1193, Accuracy: 0.9895, PPL: 1.1267, Inference Time: 1.42s\n",
      "模型和词表已保存到 F:\\poem generation\\output\\models\\transformer_poem_model2.pth\n",
      "所有指标曲线已保存到 F:\\poem generation\\output\\metrics\n",
      "类别数量过多（632），仅绘制前 10 个类别。\n",
      "混淆矩阵和 ROC 曲线已保存到 F:\\poem generation\\output\\metrics\n",
      "\n",
      "生成的诗句 (五言绝句):\n",
      "海畔尖山\n",
      "\n",
      "似宝云与真，\n",
      "如椽彼一时。\n",
      "节恐泛杯篱，\n",
      "菊少留与真。\n",
      "\n",
      "生成的诗句已保存到 F:\\poem generation\\output\\results\\海畔尖山_五言绝句_generated.txt\n",
      "\n",
      "生成的诗句 (七言绝句):\n",
      "赠君一法\n",
      "\n",
      "决浮云斩邪佞扰，\n",
      "浮生须有后信潮。\n",
      "真伪复谁知今日，\n",
      "事合随机篱菊少。\n",
      "\n",
      "生成的诗句已保存到 F:\\poem generation\\output\\results\\赠君一法_七言绝句_generated.txt\n",
      "\n",
      "生成的诗句 (七言律诗):\n",
      "怒风狂雨\n",
      "\n",
      "信悠悠悠悠悠悠，\n",
      "悠悠悠悠心知韵。\n",
      "胜处处处割愁落，\n",
      "为夏侯里擎尸出。\n",
      "东都落墨花流言，\n",
      "后信潮尤惜别心。\n",
      "性共一言后信潮，\n",
      "帽海风多蔬菜少。\n",
      "\n",
      "生成的诗句已保存到 F:\\poem generation\\output\\results\\怒风狂雨_七言律诗_generated.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 主程序入口\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置参数\n",
    "    data_path = r\"F:\\poem generation\\data\\poems_small.txt\"\n",
    "    model_save_path = r\"F:\\poem generation\\output\\models\\transformer_poem_model2.pth\"\n",
    "    metrics_path = r\"F:\\poem generation\\output\\metrics\"\n",
    "    results_path = r\"F:\\poem generation\\output\\results\"\n",
    "\n",
    "    # 训练参数\n",
    "    seq_length = 20\n",
    "    embedding_dim = 128\n",
    "    num_heads = 4\n",
    "    num_layers = 2\n",
    "    ff_dim = 512\n",
    "    epochs = 5\n",
    "    batch_size = 8\n",
    "    lr = 0.001\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"使用设备: {device}\")\n",
    "\n",
    "    # 训练模型\n",
    "    train_transformer(data_path, model_save_path, metrics_path, seq_length, embedding_dim, num_heads, \n",
    "                      num_layers, ff_dim, epochs, batch_size, lr, device)\n",
    "\n",
    "    # 加载模型和词表\n",
    "    vocab = torch.load(model_save_path.replace('.pth', '_vocab.pth'))\n",
    "    char2idx, idx2char = vocab['char2idx'], vocab['idx2char']\n",
    "    \n",
    "    model = CharLevelTransformer(len(char2idx), embedding_dim, num_heads, num_layers, ff_dim, seq_length).to(device)\n",
    "    model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "    # 生成文本示例\n",
    "    start_texts = [\"海畔尖山\", \"赠君一法\", \"怒风狂雨\"]\n",
    "    poem_types = [\"五言绝句\", \"七言绝句\", \"七言律诗\"]\n",
    "    for start, poem_type in zip(start_texts, poem_types):\n",
    "        generated = generate_text(model, start, char2idx, idx2char, seq_length, \n",
    "                                  length=50, poem_type=poem_type, temperature=0.7, device=device)\n",
    "        print(f\"\\n生成的诗句 ({poem_type}):\")\n",
    "        print(generated)\n",
    "\n",
    "        # 保存生成结果\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        result_file = os.path.join(results_path, f\"{start}_{poem_type}_generated.txt\")\n",
    "        with open(result_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(generated)\n",
    "        print(f\"生成的诗句已保存到 {result_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "生成的诗句 (五言绝句):\n",
      "海畔尖山\n",
      "\n",
      "似宝祚遥与，\n",
      "祝蓍河海风。\n",
      "多蔬菜少留，\n",
      "天上乐留与。\n",
      "\n",
      "生成的诗句已保存到 F:\\poem generation\\output\\results\\海畔尖山_五言绝句_generated.txt\n",
      "\n",
      "生成的诗句 (七言绝句):\n",
      "赠君一法\n",
      "\n",
      "决狐疑长恐泛杯，\n",
      "篱菊少许史家知。\n",
      "今日满城牵挽问，\n",
      "衙探胜处处处处。\n",
      "\n",
      "生成的诗句已保存到 F:\\poem generation\\output\\results\\赠君一法_七言绝句_generated.txt\n",
      "\n",
      "生成的诗句 (七言律诗):\n",
      "怒风狂雨\n",
      "\n",
      "烧午寝时错用情，\n",
      "尤惜别时错用情。\n",
      "尤惜别入薜萝流，\n",
      "年期雪连三日遒。\n",
      "爱幽深入薜萝谦，\n",
      "恭未篡时错用情。\n",
      "尤惜别死共一言，\n",
      "后信潮山怜峻极。\n",
      "\n",
      "生成的诗句已保存到 F:\\poem generation\\output\\results\\怒风狂雨_七言律诗_generated.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 加载保存的模型和词表\n",
    "model_save_path = r\"F:\\poem generation\\output\\models\\transformer_poem_model2.pth\"\n",
    "vocab_path = model_save_path.replace('.pth', '_vocab.pth')\n",
    "\n",
    "# 加载词表\n",
    "vocab = torch.load(vocab_path)\n",
    "char2idx, idx2char = vocab['char2idx'], vocab['idx2char']\n",
    "\n",
    "# 加载模型\n",
    "seq_length = 20  # 设置与训练时相同的序列长度\n",
    "embedding_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "ff_dim = 512\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = CharLevelTransformer(len(char2idx), embedding_dim, num_heads, num_layers, ff_dim, seq_length).to(device)\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "    # 生成文本示例\n",
    "start_texts = [\"海畔尖山\", \"赠君一法\", \"怒风狂雨\"]\n",
    "poem_types = [\"五言绝句\", \"七言绝句\", \"七言律诗\"]\n",
    "for start, poem_type in zip(start_texts, poem_types):\n",
    "        generated = generate_text(model, start, char2idx, idx2char, seq_length, \n",
    "                                  length=50, poem_type=poem_type, temperature=0.7, device=device)\n",
    "        print(f\"\\n生成的诗句 ({poem_type}):\")\n",
    "        print(generated)\n",
    "\n",
    "        # 保存生成结果\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        result_file = os.path.join(results_path, f\"{start}_{poem_type}_generated.txt\")\n",
    "        with open(result_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(generated)\n",
    "        print(f\"生成的诗句已保存到 {result_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-Gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
